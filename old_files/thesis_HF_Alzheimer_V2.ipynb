{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9b4d82",
   "metadata": {},
   "source": [
    "# KAN: Kolmogorov–Arnold Networks With Resnet18\n",
    "\n",
    "Implementation of KAN architecture with proper feature extraction and attention mechanisms for MRI image analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f09379",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "\n",
    "Import required libraries and modules for implementing KAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c175f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8335a57",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Setup data pipelines with proper transforms for MRI images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f4fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data directories\n",
    "# TRAIN_DIR = \"/home/mhs/thesis/Brain MRI ND-5 Dataset/tumordata/Training\"\n",
    "# TEST_DIR  = \"/home/mhs/thesis/Brain MRI ND-5 Dataset/tumordata/Testing\"\n",
    "\n",
    "# # Define image transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "#     transforms.Resize((224, 224)),                # Resize for backbone\n",
    "#     transforms.ToTensor(),                        # Convert to tensor\n",
    "#     transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Repeat grayscale to 3 channels\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                        std=[0.229, 0.224, 0.225])   # ImageNet normalization\n",
    "# ])\n",
    "\n",
    "# # Create datasets\n",
    "# train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=transform)\n",
    "# test_dataset  = datasets.ImageFolder(TEST_DIR,  transform=transform)\n",
    "\n",
    "# # Create dataloaders\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "#                          num_workers=4, pin_memory=True)\n",
    "# test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False,\n",
    "#                          num_workers=4, pin_memory=True)\n",
    "\n",
    "# num_classes = len(train_dataset.classes)\n",
    "# print(f\"Number of classes: {num_classes}\")\n",
    "# print(f\"Training samples: {len(train_dataset)}\")\n",
    "# print(f\"Testing samples:  {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0039d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n",
      "Training samples:   5120\n",
      "Testing samples:    1280\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ─── 1) Load your HF dataset ───────────────────────────────\n",
    "ds = load_dataset(\"Falah/Alzheimer_MRI\")\n",
    "# ds[\"train\"] and ds[\"test\"] each have an \"image\" (PIL Image) and \"label\" int\n",
    "\n",
    "# ─── 2) Define exactly the same transforms ─────────────────\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),     # make sure it’s single-channel\n",
    "    transforms.Resize((224, 224)),                   # resize for your backbone\n",
    "    transforms.ToTensor(),                           # PIL → [0,1] tensor\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # expand to 3 channels\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# ─── 3) Wrap HF splits in a torch Dataset ────────────────\n",
    "class HFImageDataset(Dataset):\n",
    "    def __init__(self, hf_split, transform=None):\n",
    "        self.split = hf_split\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.split[idx]\n",
    "        img   = example[\"image\"]      # PIL.Image\n",
    "        label = example[\"label\"]      # int\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "train_dataset = HFImageDataset(ds[\"train\"], transform=transform)\n",
    "test_dataset  = HFImageDataset(ds[\"test\"],  transform=transform)\n",
    "\n",
    "# ─── 4) Create DataLoaders ────────────────────────────────\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ─── 5) Quick checks ───────────────────────────────────────\n",
    "num_classes = len(ds[\"train\"].features[\"label\"].names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n",
    "print(f\"Testing samples:    {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e755938",
   "metadata": {},
   "source": [
    "## 3. Define KAN Architecture\n",
    "\n",
    "Implement the Kernel Attention Network model with proper feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b481ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KernelAttention(nn.Module):\n",
    "#     def __init__(self, in_dim, kernel_size=7):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Conv2d(in_dim, in_dim, kernel_size=kernel_size, \n",
    "#                              padding=kernel_size//2, groups=in_dim)\n",
    "#         self.spatial_gate = nn.Sequential(\n",
    "#             nn.Conv2d(in_dim, 1, kernel_size=1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Local feature aggregation\n",
    "#         local_feat = self.conv(x)\n",
    "#         # Generate attention weights\n",
    "#         attn = self.spatial_gate(local_feat)\n",
    "#         return x * attn\n",
    "\n",
    "# class KANModel(nn.Module):\n",
    "#     def __init__(self, num_classes, backbone='resnet18'):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # 1. Feature Extraction Backbone\n",
    "#         if backbone == 'resnet18':\n",
    "#             base = models.resnet18(pretrained=True)\n",
    "#             self.feature_dim = 512\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "            \n",
    "#         # Remove the final FC layer\n",
    "#         self.features = nn.Sequential(*list(base.children())[:-2])\n",
    "        \n",
    "#         # 2. Kernel Attention Module\n",
    "#         self.attention = KernelAttention(self.feature_dim)\n",
    "        \n",
    "#         # 3. Global Average Pooling\n",
    "#         self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "#         # 4. Classifier\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(self.feature_dim, 256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Extract features\n",
    "#         x = self.features(x)  # [B, 512, H', W']\n",
    "        \n",
    "#         # Apply kernel attention\n",
    "#         x = self.attention(x)\n",
    "        \n",
    "#         # Global average pooling\n",
    "#         x = self.gap(x)      # [B, 512, 1, 1]\n",
    "#         x = x.view(x.size(0), -1)  # [B, 512]\n",
    "        \n",
    "#         # Classification\n",
    "#         return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726189d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSpline(nn.Module):\n",
    "    \"\"\"B-spline implementation for KAN.\"\"\"\n",
    "    def __init__(self, in_dim, grid_size=5, degree=3):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.grid_size = grid_size\n",
    "        self.degree = degree\n",
    "        \n",
    "        # Learnable control points\n",
    "        self.control_points = nn.Parameter(torch.randn(in_dim, grid_size))\n",
    "        \n",
    "        # Fixed grid points from 0 to 1\n",
    "        self.register_buffer('grid', torch.linspace(0, 1, grid_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, N]\n",
    "        B, C, N = x.shape\n",
    "        \n",
    "        # Normalize input to [0, 1]\n",
    "        x_norm = (x - x.min(dim=2, keepdim=True)[0]) / (x.max(dim=2, keepdim=True)[0] - x.min(dim=2, keepdim=True)[0] + 1e-8)\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        out = torch.zeros_like(x)\n",
    "        \n",
    "        # Compute B-spline weights for each channel\n",
    "        for c in range(C):\n",
    "            # Expand grid for broadcasting\n",
    "            grid_expanded = self.grid.view(1, -1)  # [1, grid_size]\n",
    "            x_expanded = x_norm[:, c, :].unsqueeze(2)  # [B, N, 1]\n",
    "            \n",
    "            # Compute weights using RBF\n",
    "            weights = torch.exp(-((x_expanded - grid_expanded) ** 2) / 0.1)  # [B, N, grid_size]\n",
    "            weights = weights / (weights.sum(dim=2, keepdim=True) + 1e-8)\n",
    "            \n",
    "            # Apply weights to control points\n",
    "            out[:, c, :] = torch.matmul(weights, self.control_points[c].unsqueeze(1)).squeeze(2)\n",
    "            \n",
    "        return out\n",
    "\n",
    "class KANAttention(nn.Module):\n",
    "    def __init__(self, in_dim, grid_size=5, degree=3):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        # Modified KAN layer\n",
    "        self.spline = BSpline(in_dim, grid_size, degree)\n",
    "        self.attention_conv = nn.Conv1d(in_dim, 1, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Reshape to [B, C, H*W]\n",
    "        x_flat = x.view(B, C, -1)\n",
    "        \n",
    "        # Apply B-spline transformation\n",
    "        x_spline = self.spline(x_flat)\n",
    "        \n",
    "        # Generate attention weights\n",
    "        attn = self.attention_conv(x_spline)  # [B, 1, H*W]\n",
    "        attn = self.activation(attn)\n",
    "        \n",
    "        # Reshape attention back to spatial dimensions\n",
    "        attn = attn.view(B, 1, H, W)\n",
    "        \n",
    "        # Apply attention\n",
    "        return x * attn.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835b00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANModel(nn.Module):\n",
    "    def __init__(self, num_classes, backbone='resnet18'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Feature Extraction Backbone\n",
    "        if backbone == 'resnet18':\n",
    "            base = models.resnet18(pretrained=True)\n",
    "            self.feature_dim = 512\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "            \n",
    "        # Remove final FC layer and keep feature extractor\n",
    "        self.features = nn.Sequential(*list(base.children())[:-2])\n",
    "        \n",
    "        # 2. KAN Attention Module\n",
    "        self.attention = KANAttention(self.feature_dim)\n",
    "        \n",
    "        # 3. Global Average Pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # 4. Final Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.features(x)  # [B, 512, H', W']\n",
    "        \n",
    "        # Apply KAN attention\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.gap(x)      # [B, 512, 1, 1]\n",
    "        x = x.view(x.size(0), -1)  # [B, 512]\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c6802",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "Setup training parameters and optimization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbbcdfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhs/miniforge3/envs/thesis/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mhs/miniforge3/envs/thesis/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = KANModel(num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler (removed verbose parameter)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409394b9",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation Loop\n",
    "\n",
    "Implement the main training loop with evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0bca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [00:43<00:00,  3.71it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 15.36it/s]\n",
      "/home/mhs/miniforge3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mhs/miniforge3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mhs/miniforge3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.9503, Train Acc: 0.6326\n",
      "Val Loss: 0.5817, Val Acc: 0.8047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [00:42<00:00,  3.76it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 17.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.3014, Train Acc: 0.9148\n",
      "Val Loss: 0.3925, Val Acc: 0.8562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [00:42<00:00,  3.80it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.0921, Train Acc: 0.9840\n",
      "Val Loss: 0.3787, Val Acc: 0.8539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [00:44<00:00,  3.62it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.0739, Train Acc: 0.9859\n",
      "Val Loss: 0.2509, Val Acc: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [00:42<00:00,  3.74it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:55<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0626, Train Acc: 0.9873\n",
      "Val Loss: 0.4840, Val Acc: 0.8375\n",
      "\n",
      "Validation Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Mild_Demented       0.96      0.67      0.79       172\n",
      " Moderate_Demented       1.00      0.67      0.80        15\n",
      "      Non_Demented       0.76      1.00      0.86       634\n",
      "Very_Mild_Demented       0.98      0.69      0.81       459\n",
      "\n",
      "          accuracy                           0.84      1280\n",
      "         macro avg       0.92      0.75      0.81      1280\n",
      "      weighted avg       0.87      0.84      0.83      1280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [-1:59:50<00:00, -15.87it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0595, Train Acc: 0.9850\n",
      "Val Loss: 0.5378, Val Acc: 0.8133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [00:53<00:00,  3.02it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0453, Train Acc: 0.9889\n",
      "Val Loss: 0.1290, Val Acc: 0.9570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [00:43<00:00,  3.66it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 15.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0399, Train Acc: 0.9904\n",
      "Val Loss: 0.1387, Val Acc: 0.9570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [01:29<00:00,  1.79it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0360, Train Acc: 0.9904\n",
      "Val Loss: 0.1137, Val Acc: 0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 160/160 [01:17<00:00,  2.05it/s]\n",
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0270, Train Acc: 0.9936\n",
      "Val Loss: 0.1035, Val Acc: 0.9586\n",
      "\n",
      "Validation Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Mild_Demented       0.93      0.96      0.95       172\n",
      " Moderate_Demented       1.00      0.93      0.97        15\n",
      "      Non_Demented       1.00      0.94      0.97       634\n",
      "Very_Mild_Demented       0.92      0.98      0.95       459\n",
      "\n",
      "          accuracy                           0.96      1280\n",
      "         macro avg       0.96      0.95      0.96      1280\n",
      "      weighted avg       0.96      0.96      0.96      1280\n",
      "\n",
      "\n",
      "Loading best model for final evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 40/40 [00:02<00:00, 17.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Results:\n",
      "Test Accuracy: 0.9625\n",
      "\n",
      "Detailed Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Mild_Demented       0.96      0.92      0.94       172\n",
      " Moderate_Demented       1.00      0.93      0.97        15\n",
      "      Non_Demented       0.98      0.96      0.97       634\n",
      "Very_Mild_Demented       0.94      0.98      0.96       459\n",
      "\n",
      "          accuracy                           0.96      1280\n",
      "         macro avg       0.97      0.95      0.96      1280\n",
      "      weighted avg       0.96      0.96      0.96      1280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc='Training'):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Evaluating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    classes=ds[\"train\"].features[\"label\"].names\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                target_names=classes)\n",
    "    \n",
    "    return avg_loss, accuracy, report\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Training phase\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, \n",
    "                                      optimizer, device)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    val_loss, val_acc, val_report = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'kan_best_model.pth')\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Print detailed validation report every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"\\nValidation Report:\")\n",
    "        print(val_report)\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nLoading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load('kan_best_model.pth'))\n",
    "test_loss, test_acc, test_report = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e6b2b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Modify the training loop to store metrics\n",
    "# train_losses = []\n",
    "# train_accs = []\n",
    "# val_losses = []\n",
    "# val_accs = []\n",
    "\n",
    "# for epoch in range(1, num_epochs + 1):\n",
    "#     # Training phase\n",
    "#     train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "#     train_losses.append(train_loss)\n",
    "#     train_accs.append(train_acc)\n",
    "    \n",
    "#     # Evaluation phase\n",
    "#     val_loss, val_acc, val_report = evaluate(model, test_loader, criterion, device)\n",
    "#     val_losses.append(val_loss)\n",
    "#     val_accs.append(val_acc)\n",
    "    \n",
    "#     # ... rest of your existing training loop code ...\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# # Plot Loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(range(1, num_epochs + 1), train_losses, 'b-', label='Training Loss')\n",
    "# plt.plot(range(1, num_epochs + 1), val_losses, 'r-', label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Plot Accuracy\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(range(1, num_epochs + 1), train_accs, 'b-', label='Training Accuracy')\n",
    "# plt.plot(range(1, num_epochs + 1), val_accs, 'r-', label='Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d174384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
