{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce230fc4",
   "metadata": {
    "executionInfo": {
     "elapsed": 8528,
     "status": "ok",
     "timestamp": 1745909083732,
     "user": {
      "displayName": "Md. Mehedi Hasan Shawon",
      "userId": "13305223100191807763"
     },
     "user_tz": -360
    },
    "id": "ce230fc4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5017b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6367 images\n",
      "Val:   1364 images\n",
      "Test:  1365 images\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# 1) point this at your base folder, e.g. \"/path/to/output_dir\"\n",
    "data_dir = \"/home/mhs/research/thesis/balanced_img\"\n",
    "\n",
    "# 2) define any transforms you need (resize + to-tensor here)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 3) load the full dataset; ImageFolder will assign class-indices based on subdir names\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# 4) compute split sizes\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.70 * total_size)\n",
    "val_size   = int(0.15 * total_size)\n",
    "test_size  = total_size - train_size - val_size\n",
    "\n",
    "# 5) do the split (set a seed for reproducibility)\n",
    "train_ds, val_ds, test_ds = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# 6) wrap in DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# now `train_loader`, `val_loader`, `test_loader` yield (images, labels) batches\n",
    "print(f\"Train: {len(train_ds)} images\\nVal:   {len(val_ds)} images\\nTest:  {len(test_ds)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1c57d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape  # check the shape of a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d25b58b",
   "metadata": {
    "id": "3d25b58b",
    "outputId": "1355611e-3041-4f36-b19e-43d6f0fa7306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "NCMQ-IAmDNUn",
   "metadata": {
    "id": "NCMQ-IAmDNUn",
    "outputId": "a0fa402f-51e8-4d0b-b523-1c7f933594f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 18:01:32,414] A new study created in memory with name: no-name-f84a622c-fd85-45f2-a76d-3225fdbbdea4\n",
      "/tmp/ipykernel_146258/3697293579.py:76: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  self.dropout_p = trial.suggest_uniform('dropout', 0.0, 0.5)\n",
      "/tmp/ipykernel_146258/3697293579.py:112: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
      "/tmp/ipykernel_146258/3697293579.py:118: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  momentum = trial.suggest_uniform('momentum', 0.5, 0.9)\n",
      "Training Progress: 100%|██████████| 28/28 [01:00<00:00,  2.15s/it]\n",
      "[I 2025-04-29 18:02:32,909] Trial 0 finished with value: 1.3788696250250174 and parameters: {'n_conv_layers': 4, 'conv0_out': 128, 'conv0_k': 3, 'conv0_pad': 0, 'conv0_norm': 'none', 'conv1_out': 32, 'conv1_k': 7, 'conv1_pad': 2, 'conv1_norm': 'none', 'conv2_out': 128, 'conv2_k': 3, 'conv2_pad': 1, 'conv2_norm': 'layer', 'conv3_out': 16, 'conv3_k': 3, 'conv3_pad': 0, 'conv3_norm': 'batch', 'n_fc_layers': 3, 'fc0_units': 64, 'fc1_units': 288, 'fc2_units': 160, 'dropout': 0.3529545614880699, 'activation': 'leaky_relu', 'optimizer': 'SGD', 'lr': 3.139804881067671e-05, 'momentum': 0.6864443145682289, 'epochs': 28, 'patience': 5}. Best is trial 0 with value: 1.3788696250250174.\n",
      "Training Progress:  38%|███▊      | 8/21 [00:19<00:31,  2.41s/it]\n",
      "[I 2025-04-29 18:02:52,267] Trial 1 finished with value: 1.1234418458716815 and parameters: {'n_conv_layers': 2, 'conv0_out': 16, 'conv0_k': 7, 'conv0_pad': 3, 'conv0_norm': 'batch', 'conv1_out': 32, 'conv1_k': 7, 'conv1_pad': 0, 'conv1_norm': 'batch', 'n_fc_layers': 3, 'fc0_units': 128, 'fc1_units': 512, 'fc2_units': 288, 'dropout': 0.41054693362124994, 'activation': 'elu', 'optimizer': 'RMSprop', 'lr': 6.933258655706183e-05, 'epochs': 21, 'patience': 4}. Best is trial 1 with value: 1.1234418458716815.\n",
      "Training Progress: 100%|██████████| 13/13 [00:28<00:00,  2.16s/it]\n",
      "[I 2025-04-29 18:03:20,484] Trial 2 finished with value: 1.1170669600021008 and parameters: {'n_conv_layers': 4, 'conv0_out': 128, 'conv0_k': 5, 'conv0_pad': 1, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 0, 'conv1_norm': 'layer', 'conv2_out': 32, 'conv2_k': 7, 'conv2_pad': 2, 'conv2_norm': 'layer', 'conv3_out': 32, 'conv3_k': 5, 'conv3_pad': 1, 'conv3_norm': 'none', 'n_fc_layers': 2, 'fc0_units': 416, 'fc1_units': 32, 'dropout': 0.350293404476256, 'activation': 'elu', 'optimizer': 'Adam', 'lr': 1.4472730238682082e-05, 'epochs': 13, 'patience': 3}. Best is trial 2 with value: 1.1170669600021008.\n",
      "Training Progress: 100%|██████████| 42/42 [01:30<00:00,  2.15s/it]\n",
      "[I 2025-04-29 18:04:50,762] Trial 3 finished with value: 1.165965706803078 and parameters: {'n_conv_layers': 2, 'conv0_out': 16, 'conv0_k': 5, 'conv0_pad': 0, 'conv0_norm': 'batch', 'conv1_out': 16, 'conv1_k': 7, 'conv1_pad': 0, 'conv1_norm': 'none', 'n_fc_layers': 2, 'fc0_units': 512, 'fc1_units': 96, 'dropout': 0.4052817774183464, 'activation': 'elu', 'optimizer': 'SGD', 'lr': 0.00018534289634611022, 'momentum': 0.7187626576084853, 'epochs': 42, 'patience': 4}. Best is trial 2 with value: 1.1170669600021008.\n",
      "Training Progress:  62%|██████▎   | 20/32 [00:45<00:27,  2.25s/it]\n",
      "[I 2025-04-29 18:05:35,912] Trial 4 finished with value: 1.0911814681319303 and parameters: {'n_conv_layers': 2, 'conv0_out': 32, 'conv0_k': 5, 'conv0_pad': 1, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 2, 'conv1_norm': 'none', 'n_fc_layers': 2, 'fc0_units': 352, 'fc1_units': 448, 'dropout': 0.19422777962874738, 'activation': 'relu', 'optimizer': 'SGD', 'lr': 0.0027900462372203233, 'momentum': 0.5564941145841625, 'epochs': 32, 'patience': 6}. Best is trial 4 with value: 1.0911814681319303.\n",
      "Training Progress:  37%|███▋      | 14/38 [00:32<00:55,  2.31s/it]\n",
      "[I 2025-04-29 18:06:08,422] Trial 5 finished with value: 1.1317076877106067 and parameters: {'n_conv_layers': 4, 'conv0_out': 32, 'conv0_k': 7, 'conv0_pad': 3, 'conv0_norm': 'batch', 'conv1_out': 16, 'conv1_k': 5, 'conv1_pad': 2, 'conv1_norm': 'batch', 'conv2_out': 32, 'conv2_k': 3, 'conv2_pad': 1, 'conv2_norm': 'none', 'conv3_out': 32, 'conv3_k': 3, 'conv3_pad': 0, 'conv3_norm': 'none', 'n_fc_layers': 2, 'fc0_units': 256, 'fc1_units': 480, 'dropout': 0.48403767578546425, 'activation': 'elu', 'optimizer': 'RMSprop', 'lr': 8.06114507066104e-05, 'epochs': 38, 'patience': 5}. Best is trial 4 with value: 1.0911814681319303.\n",
      "Training Progress:   0%|          | 0/48 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:06:10,718] Trial 6 pruned. \n",
      "Training Progress:  53%|█████▎    | 9/17 [00:21<00:19,  2.40s/it]\n",
      "[I 2025-04-29 18:06:32,422] Trial 7 pruned. \n",
      "Training Progress:   0%|          | 0/32 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:06:34,709] Trial 8 pruned. \n",
      "Training Progress:   0%|          | 0/11 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:06:36,983] Trial 9 pruned. \n",
      "Training Progress:   0%|          | 0/26 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:06:39,299] Trial 10 pruned. \n",
      "Training Progress:   0%|          | 0/10 [00:03<?, ?it/s]\n",
      "[I 2025-04-29 18:06:42,867] Trial 11 pruned. \n",
      "Training Progress:   0%|          | 0/33 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:06:45,191] Trial 12 pruned. \n",
      "Training Progress:  65%|██████▌   | 13/20 [00:30<00:16,  2.32s/it]\n",
      "[I 2025-04-29 18:07:15,515] Trial 13 finished with value: 1.052528040353642 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 448, 'dropout': 0.3189633606992345, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.0007517468850090775, 'epochs': 20, 'patience': 6}. Best is trial 13 with value: 1.052528040353642.\n",
      "Training Progress:   5%|▍         | 1/22 [00:04<01:30,  4.30s/it]\n",
      "[I 2025-04-29 18:07:19,951] Trial 14 pruned. \n",
      "Training Progress:   0%|          | 0/38 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:07:22,260] Trial 15 pruned. \n",
      "Training Progress:  57%|█████▋    | 13/23 [00:30<00:23,  2.33s/it]\n",
      "[I 2025-04-29 18:07:52,643] Trial 16 finished with value: 1.0630655163942382 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 320, 'dropout': 0.09704362557918722, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.0003987372844717922, 'epochs': 23, 'patience': 7}. Best is trial 13 with value: 1.052528040353642.\n",
      "Training Progress:  63%|██████▎   | 12/19 [00:28<00:16,  2.34s/it]\n",
      "[I 2025-04-29 18:08:20,822] Trial 17 finished with value: 1.0800839149674704 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 288, 'dropout': 0.05774232330858775, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.0003558667430643963, 'epochs': 19, 'patience': 7}. Best is trial 13 with value: 1.052528040353642.\n",
      "Training Progress:   0%|          | 0/25 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:08:23,122] Trial 18 pruned. \n",
      "Training Progress:   0%|          | 0/17 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:08:25,421] Trial 19 pruned. \n",
      "Training Progress:  80%|████████  | 12/15 [00:27<00:06,  2.33s/it]\n",
      "[I 2025-04-29 18:08:53,459] Trial 20 finished with value: 1.090284104957137 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 16, 'conv1_k': 3, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 288, 'dropout': 0.007334734132725257, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.0005648468907094605, 'epochs': 15, 'patience': 7}. Best is trial 13 with value: 1.052528040353642.\n",
      "Training Progress:  52%|█████▏    | 11/21 [00:25<00:23,  2.35s/it]\n",
      "[I 2025-04-29 18:09:19,424] Trial 21 finished with value: 1.0607196591621222 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 288, 'dropout': 0.10236556627965873, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.0002611935933010823, 'epochs': 21, 'patience': 7}. Best is trial 13 with value: 1.052528040353642.\n",
      "Training Progress:  46%|████▌     | 11/24 [00:25<00:30,  2.35s/it]\n",
      "[I 2025-04-29 18:09:45,377] Trial 22 finished with value: 1.0574856608412986 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 224, 'dropout': 0.11189458539835098, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.00027138087920198345, 'epochs': 24, 'patience': 7}. Best is trial 13 with value: 1.052528040353642.\n",
      "Training Progress:   4%|▎         | 1/28 [00:04<01:56,  4.30s/it]\n",
      "[I 2025-04-29 18:09:49,818] Trial 23 pruned. \n",
      "Training Progress:   0%|          | 0/20 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:09:52,111] Trial 24 pruned. \n",
      "Training Progress:   4%|▍         | 1/24 [00:04<01:39,  4.32s/it]\n",
      "[I 2025-04-29 18:09:56,575] Trial 25 pruned. \n",
      "Training Progress:  93%|█████████▎| 14/15 [00:32<00:02,  2.32s/it]\n",
      "[I 2025-04-29 18:10:29,139] Trial 26 finished with value: 1.0471430933752726 and parameters: {'n_conv_layers': 4, 'conv0_out': 64, 'conv0_k': 7, 'conv0_pad': 3, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 5, 'conv2_pad': 0, 'conv2_norm': 'batch', 'conv3_out': 128, 'conv3_k': 5, 'conv3_pad': 2, 'conv3_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 96, 'dropout': 0.1352504254857248, 'activation': 'relu', 'optimizer': 'RMSprop', 'lr': 0.0003554582958542614, 'epochs': 15, 'patience': 7}. Best is trial 26 with value: 1.0471430933752726.\n",
      "Training Progress:   0%|          | 0/18 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:10:31,459] Trial 27 pruned. \n",
      "Training Progress:   0%|          | 0/16 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:10:33,771] Trial 28 pruned. \n",
      "Training Progress:   0%|          | 0/29 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:10:36,075] Trial 29 pruned. \n",
      "Training Progress:  93%|█████████▎| 13/14 [00:30<00:02,  2.33s/it]\n",
      "[I 2025-04-29 18:11:06,538] Trial 30 finished with value: 1.0651907893114312 and parameters: {'n_conv_layers': 4, 'conv0_out': 64, 'conv0_k': 7, 'conv0_pad': 3, 'conv0_norm': 'none', 'conv1_out': 128, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'batch', 'conv2_out': 16, 'conv2_k': 5, 'conv2_pad': 0, 'conv2_norm': 'batch', 'conv3_out': 64, 'conv3_k': 5, 'conv3_pad': 2, 'conv3_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 32, 'dropout': 0.1492735187839572, 'activation': 'relu', 'optimizer': 'RMSprop', 'lr': 0.0001523667603751813, 'epochs': 14, 'patience': 7}. Best is trial 26 with value: 1.0471430933752726.\n",
      "Training Progress:  48%|████▊     | 13/27 [00:30<00:32,  2.33s/it]\n",
      "[I 2025-04-29 18:11:36,965] Trial 31 finished with value: 1.0622780142828476 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 192, 'dropout': 0.12742978142368425, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.0003169510149414738, 'epochs': 27, 'patience': 7}. Best is trial 26 with value: 1.0471430933752726.\n",
      "Training Progress:  48%|████▊     | 10/21 [00:23<00:26,  2.38s/it]\n",
      "[I 2025-04-29 18:12:00,899] Trial 32 finished with value: 1.071489623812742 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 2, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 128, 'dropout': 0.192712482534325, 'activation': 'relu', 'optimizer': 'RMSprop', 'lr': 0.0003159300369409104, 'epochs': 21, 'patience': 7}. Best is trial 26 with value: 1.0471430933752726.\n",
      "Training Progress:   0%|          | 0/13 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:12:03,209] Trial 33 pruned. \n",
      "Training Progress:   0%|          | 0/20 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:12:05,521] Trial 34 pruned. \n",
      "Training Progress:   5%|▍         | 1/22 [00:04<01:31,  4.37s/it]\n",
      "[I 2025-04-29 18:12:10,029] Trial 35 pruned. \n",
      "Training Progress:  42%|████▏     | 10/24 [00:23<00:33,  2.38s/it]\n",
      "[I 2025-04-29 18:12:34,008] Trial 36 finished with value: 1.1037178164304688 and parameters: {'n_conv_layers': 2, 'conv0_out': 128, 'conv0_k': 3, 'conv0_pad': 0, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'none', 'n_fc_layers': 2, 'fc0_units': 352, 'fc1_units': 192, 'dropout': 0.3277178290910617, 'activation': 'elu', 'optimizer': 'Adam', 'lr': 4.442301222731701e-05, 'epochs': 24, 'patience': 7}. Best is trial 26 with value: 1.0471430933752726.\n",
      "Training Progress:   0%|          | 0/12 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:12:36,322] Trial 37 pruned. \n",
      "Training Progress:   6%|▌         | 1/18 [00:04<01:13,  4.31s/it]\n",
      "[I 2025-04-29 18:12:40,769] Trial 38 pruned. \n",
      "Training Progress:   0%|          | 0/50 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:12:43,073] Trial 39 pruned. \n",
      "Training Progress:   0%|          | 0/34 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:12:45,379] Trial 40 pruned. \n",
      "Training Progress:  41%|████      | 11/27 [00:26<00:37,  2.37s/it]\n",
      "[I 2025-04-29 18:13:11,545] Trial 41 finished with value: 1.0608521325643672 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 192, 'dropout': 0.20573840619413586, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.00020098007422401684, 'epochs': 27, 'patience': 7}. Best is trial 26 with value: 1.0471430933752726.\n",
      "Training Progress:  47%|████▋     | 14/30 [00:32<00:37,  2.33s/it]\n",
      "[I 2025-04-29 18:13:44,252] Trial 42 finished with value: 1.0800750962523527 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 128, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 160, 'dropout': 0.22045067071146993, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.00023936108131455822, 'epochs': 30, 'patience': 7}. Best is trial 26 with value: 1.0471430933752726.\n",
      "Training Progress:  38%|███▊      | 10/26 [00:23<00:38,  2.38s/it]\n",
      "[I 2025-04-29 18:14:08,197] Trial 43 finished with value: 1.0834009328553842 and parameters: {'n_conv_layers': 3, 'conv0_out': 64, 'conv0_k': 5, 'conv0_pad': 2, 'conv0_norm': 'batch', 'conv1_out': 64, 'conv1_k': 5, 'conv1_pad': 1, 'conv1_norm': 'layer', 'conv2_out': 16, 'conv2_k': 7, 'conv2_pad': 3, 'conv2_norm': 'batch', 'n_fc_layers': 1, 'fc0_units': 256, 'dropout': 0.19025583036962862, 'activation': 'relu', 'optimizer': 'Adam', 'lr': 0.00021276702783747757, 'epochs': 26, 'patience': 7}. Best is trial 26 with value: 1.0471430933752726.\n",
      "Training Progress:   0%|          | 0/30 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:14:10,501] Trial 44 pruned. \n",
      "Training Progress:   0%|          | 0/15 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:14:12,814] Trial 45 pruned. \n",
      "Training Progress:   0%|          | 0/35 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:14:15,113] Trial 46 pruned. \n",
      "Training Progress:   0%|          | 0/43 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:14:17,414] Trial 47 pruned. \n",
      "Training Progress:   0%|          | 0/22 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:14:19,723] Trial 48 pruned. \n",
      "Training Progress:   0%|          | 0/25 [00:02<?, ?it/s]\n",
      "[I 2025-04-29 18:14:22,051] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 1.0471430933752726\n",
      "  Params: \n",
      "    n_conv_layers: 4\n",
      "    conv0_out: 64\n",
      "    conv0_k: 7\n",
      "    conv0_pad: 3\n",
      "    conv0_norm: batch\n",
      "    conv1_out: 64\n",
      "    conv1_k: 5\n",
      "    conv1_pad: 1\n",
      "    conv1_norm: layer\n",
      "    conv2_out: 16\n",
      "    conv2_k: 5\n",
      "    conv2_pad: 0\n",
      "    conv2_norm: batch\n",
      "    conv3_out: 128\n",
      "    conv3_k: 5\n",
      "    conv3_pad: 2\n",
      "    conv3_norm: batch\n",
      "    n_fc_layers: 1\n",
      "    fc0_units: 96\n",
      "    dropout: 0.1352504254857248\n",
      "    activation: relu\n",
      "    optimizer: RMSprop\n",
      "    lr: 0.0003554582958542614\n",
      "    epochs: 15\n",
      "    patience: 7\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "class OptunaCNN(nn.Module):\n",
    "    def __init__(self, trial, num_classes=2, input_shape=None):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        if input_shape is None:\n",
    "            sample_batch, _ = next(iter(train_loader))\n",
    "            input_shape = sample_batch.shape[1:]\n",
    "\n",
    "        in_channels = input_shape[0]\n",
    "        current_size = input_shape[1]  # Track the feature map size\n",
    "        n_conv = trial.suggest_int('n_conv_layers', 1, 4)\n",
    "        \n",
    "        for i in range(n_conv):\n",
    "            # Calculate maximum valid kernel size\n",
    "            max_kernel = min(7, current_size)\n",
    "            if max_kernel % 2 == 0:  # Ensure odd kernel size\n",
    "                max_kernel -= 1\n",
    "            \n",
    "            # Only suggest kernel sizes that are valid\n",
    "            if max_kernel < 3:\n",
    "                break  # Stop adding layers if feature map is too small\n",
    "                \n",
    "            out_channels = trial.suggest_categorical(f\"conv{i}_out\", [16, 32, 64, 128])\n",
    "            k = trial.suggest_int(f\"conv{i}_k\", 3, max_kernel, step=2)\n",
    "            p = trial.suggest_int(f\"conv{i}_pad\", 0, k//2)\n",
    "            \n",
    "            self.convs.append(nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=p))\n",
    "            \n",
    "            norm_type = trial.suggest_categorical(f\"conv{i}_norm\", ['none', 'batch', 'layer'])\n",
    "            if norm_type == 'batch':\n",
    "                self.norms.append(nn.BatchNorm2d(out_channels))\n",
    "            elif norm_type == 'layer':\n",
    "                self.norms.append(nn.GroupNorm(1, out_channels))\n",
    "            else:\n",
    "                self.norms.append(None)\n",
    "                \n",
    "            in_channels = out_channels\n",
    "            # Update feature map size after conv and pool\n",
    "            current_size = (current_size + 2*p - k + 1) // 2  # Account for pooling\n",
    "            \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Rest of initialization remains the same\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, *input_shape)\n",
    "            for conv, norm in zip(self.convs, self.norms):\n",
    "                x = conv(x)\n",
    "                if norm: x = norm(x)\n",
    "                x = F.relu(x)\n",
    "                x = self.pool(x)\n",
    "            self.n_flatten = x.numel()\n",
    "\n",
    "        self.fcs = nn.ModuleList()\n",
    "        n_fc = trial.suggest_int('n_fc_layers', 1, 3)\n",
    "        in_features = self.n_flatten\n",
    "        for j in range(n_fc):\n",
    "            out_feat = trial.suggest_int(f\"fc{j}_units\", 32, 512, step=32)\n",
    "            self.fcs.append(nn.Linear(in_features, out_feat))\n",
    "            in_features = out_feat\n",
    "        self.output = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        self.dropout_p = trial.suggest_uniform('dropout', 0.0, 0.5)\n",
    "        self.act_name = trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'elu'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x)\n",
    "            if norm:\n",
    "                x = norm(x)\n",
    "            if self.act_name == 'relu':\n",
    "                x = F.relu(x)\n",
    "            elif self.act_name == 'leaky_relu':\n",
    "                x = F.leaky_relu(x)\n",
    "            elif self.act_name == 'elu':\n",
    "                x = F.elu(x)\n",
    "            x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for fc in self.fcs:\n",
    "            x = fc(x)\n",
    "            if self.act_name == 'relu':\n",
    "                x = F.relu(x)\n",
    "            elif self.act_name == 'leaky_relu':\n",
    "                x = F.leaky_relu(x)\n",
    "            elif self.act_name == 'elu':\n",
    "                x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        return self.output(x)\n",
    "\n",
    "def objective(trial):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    sample_batch, _ = next(iter(train_loader))\n",
    "    input_shape = sample_batch.shape[1:]\n",
    "\n",
    "    model = OptunaCNN(trial, num_classes=NUM_CLASSES, input_shape=input_shape).to(device)\n",
    "\n",
    "    opt_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    if opt_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif opt_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        momentum = trial.suggest_uniform('momentum', 0.5, 0.9)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    num_epochs = trial.suggest_int('epochs', 10, 50)\n",
    "    patience = trial.suggest_int('patience', 3, 7)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "NUM_CLASSES = len(full_dataset.classes)\n",
    "study = optuna.create_study(direction='minimize', pruner=MedianPruner())\n",
    "study.optimize(objective, n_trials=50, timeout=None)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, val in trial.params.items():\n",
    "    print(f\"    {key}: {val}\")\n",
    "\n",
    "# def get_img_type(img_path):\n",
    "#     if \"mri\" in file_path:\n",
    "#         return \"mri\"\n",
    "#     elif \"vg\" in file_path:\n",
    "#         return \"vg\"\n",
    "#     else:\n",
    "#         raise ValueError(\"Unknown file path. Cannot determine image type.\")\n",
    "\n",
    "# try:\n",
    "#     import yaml\n",
    "#     with open(f\"best_params_{get_img_type(file_path)}.yaml\", 'w') as f:\n",
    "#         yaml.safe_dump(trial.params, f)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error saving parameters to YAML: {e}\")\n",
    "#     with open(f\"best_params_{get_img_type(file_path)}.txt\", 'w') as f:\n",
    "#         for key, val in trial.params.items():\n",
    "#             f.write(f\"{key}: {val}\\n\")\n",
    "\n",
    "# print(\"Best parameters saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50477365",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be74889",
   "metadata": {
    "id": "2be74889",
    "outputId": "a5fdd85f-01c5-4964-9190-1af0f8206fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1.2801, Train Accuracy: 38.86%, Validation Loss: 1.1894, Validation Accuracy: 45.75%\n",
      "Epoch 2/100, Train Loss: 1.1542, Train Accuracy: 46.63%, Validation Loss: 1.1372, Validation Accuracy: 47.36%\n",
      "Epoch 3/100, Train Loss: 1.0828, Train Accuracy: 51.09%, Validation Loss: 1.0923, Validation Accuracy: 51.32%\n",
      "Epoch 4/100, Train Loss: 1.0316, Train Accuracy: 54.23%, Validation Loss: 1.0961, Validation Accuracy: 50.81%\n",
      "Epoch 5/100, Train Loss: 0.9651, Train Accuracy: 57.52%, Validation Loss: 1.1133, Validation Accuracy: 49.85%\n",
      "Epoch 6/100, Train Loss: 0.8933, Train Accuracy: 61.49%, Validation Loss: 1.1795, Validation Accuracy: 48.46%\n",
      "Epoch 7/100, Train Loss: 0.8175, Train Accuracy: 65.95%, Validation Loss: 1.0796, Validation Accuracy: 53.81%\n",
      "Epoch 8/100, Train Loss: 0.7280, Train Accuracy: 70.30%, Validation Loss: 1.2934, Validation Accuracy: 50.66%\n",
      "Epoch 9/100, Train Loss: 0.6462, Train Accuracy: 74.52%, Validation Loss: 1.2043, Validation Accuracy: 51.98%\n",
      "Epoch 10/100, Train Loss: 0.5555, Train Accuracy: 78.31%, Validation Loss: 1.2580, Validation Accuracy: 52.79%\n",
      "Epoch 11/100, Train Loss: 0.4881, Train Accuracy: 81.22%, Validation Loss: 1.2537, Validation Accuracy: 55.35%\n",
      "Epoch 12/100, Train Loss: 0.4164, Train Accuracy: 84.20%, Validation Loss: 1.1923, Validation Accuracy: 57.55%\n",
      "Epoch 13/100, Train Loss: 0.3498, Train Accuracy: 87.22%, Validation Loss: 1.4003, Validation Accuracy: 55.87%\n",
      "Epoch 14/100, Train Loss: 0.2870, Train Accuracy: 89.67%, Validation Loss: 1.6575, Validation Accuracy: 51.47%\n",
      "Epoch 15/100, Train Loss: 0.2566, Train Accuracy: 90.91%, Validation Loss: 1.5841, Validation Accuracy: 56.38%\n",
      "Epoch 16/100, Train Loss: 0.2164, Train Accuracy: 92.52%, Validation Loss: 1.5603, Validation Accuracy: 54.18%\n",
      "Epoch 17/100, Train Loss: 0.1911, Train Accuracy: 93.69%, Validation Loss: 1.8368, Validation Accuracy: 50.44%\n",
      "Epoch 18/100, Train Loss: 0.1609, Train Accuracy: 94.41%, Validation Loss: 1.7432, Validation Accuracy: 55.35%\n",
      "Epoch 19/100, Train Loss: 0.1565, Train Accuracy: 94.64%, Validation Loss: 1.8325, Validation Accuracy: 54.47%\n",
      "Epoch 20/100, Train Loss: 0.1407, Train Accuracy: 94.77%, Validation Loss: 2.2036, Validation Accuracy: 48.83%\n",
      "Epoch 21/100, Train Loss: 0.1128, Train Accuracy: 95.98%, Validation Loss: 1.8924, Validation Accuracy: 55.13%\n",
      "Epoch 22/100, Train Loss: 0.1116, Train Accuracy: 96.07%, Validation Loss: 2.6408, Validation Accuracy: 47.58%\n",
      "Epoch 23/100, Train Loss: 0.0965, Train Accuracy: 96.95%, Validation Loss: 2.6160, Validation Accuracy: 47.29%\n",
      "Epoch 24/100, Train Loss: 0.1031, Train Accuracy: 96.40%, Validation Loss: 3.1753, Validation Accuracy: 48.09%\n",
      "Epoch 25/100, Train Loss: 0.0854, Train Accuracy: 97.27%, Validation Loss: 2.1933, Validation Accuracy: 50.73%\n",
      "Epoch 26/100, Train Loss: 0.0797, Train Accuracy: 97.22%, Validation Loss: 2.1317, Validation Accuracy: 54.91%\n",
      "Epoch 27/100, Train Loss: 0.0772, Train Accuracy: 97.41%, Validation Loss: 2.0612, Validation Accuracy: 56.30%\n",
      "Epoch 28/100, Train Loss: 0.0947, Train Accuracy: 96.87%, Validation Loss: 2.3470, Validation Accuracy: 54.62%\n",
      "Epoch 29/100, Train Loss: 0.0770, Train Accuracy: 97.36%, Validation Loss: 2.1831, Validation Accuracy: 55.65%\n",
      "Epoch 30/100, Train Loss: 0.0838, Train Accuracy: 97.06%, Validation Loss: 2.4999, Validation Accuracy: 51.61%\n",
      "Epoch 31/100, Train Loss: 0.0551, Train Accuracy: 98.37%, Validation Loss: 2.3287, Validation Accuracy: 52.35%\n",
      "Epoch 32/100, Train Loss: 0.0901, Train Accuracy: 97.42%, Validation Loss: 2.4456, Validation Accuracy: 50.00%\n",
      "Epoch 33/100, Train Loss: 0.0632, Train Accuracy: 98.12%, Validation Loss: 2.1702, Validation Accuracy: 56.60%\n",
      "Epoch 34/100, Train Loss: 0.0628, Train Accuracy: 98.10%, Validation Loss: 2.7010, Validation Accuracy: 52.57%\n",
      "Epoch 35/100, Train Loss: 0.0630, Train Accuracy: 97.88%, Validation Loss: 2.3639, Validation Accuracy: 54.77%\n",
      "Epoch 36/100, Train Loss: 0.0550, Train Accuracy: 98.29%, Validation Loss: 2.5348, Validation Accuracy: 54.77%\n",
      "Epoch 37/100, Train Loss: 0.0592, Train Accuracy: 98.08%, Validation Loss: 2.1569, Validation Accuracy: 55.65%\n",
      "Epoch 38/100, Train Loss: 0.0518, Train Accuracy: 98.48%, Validation Loss: 2.9091, Validation Accuracy: 51.39%\n",
      "Epoch 39/100, Train Loss: 0.0493, Train Accuracy: 98.62%, Validation Loss: 2.3634, Validation Accuracy: 57.33%\n",
      "Epoch 40/100, Train Loss: 0.0535, Train Accuracy: 98.27%, Validation Loss: 2.3687, Validation Accuracy: 58.87%\n",
      "Epoch 41/100, Train Loss: 0.0661, Train Accuracy: 97.68%, Validation Loss: 2.4232, Validation Accuracy: 56.09%\n",
      "Epoch 42/100, Train Loss: 0.0449, Train Accuracy: 98.57%, Validation Loss: 2.8573, Validation Accuracy: 51.10%\n",
      "Epoch 43/100, Train Loss: 0.0531, Train Accuracy: 98.34%, Validation Loss: 2.3340, Validation Accuracy: 54.62%\n",
      "Epoch 44/100, Train Loss: 0.0486, Train Accuracy: 98.46%, Validation Loss: 2.3901, Validation Accuracy: 55.94%\n",
      "Epoch 45/100, Train Loss: 0.0481, Train Accuracy: 98.37%, Validation Loss: 2.8729, Validation Accuracy: 53.59%\n",
      "Epoch 46/100, Train Loss: 0.0597, Train Accuracy: 98.37%, Validation Loss: 2.7350, Validation Accuracy: 51.76%\n",
      "Epoch 47/100, Train Loss: 0.0537, Train Accuracy: 98.40%, Validation Loss: 2.3536, Validation Accuracy: 55.50%\n",
      "Epoch 48/100, Train Loss: 0.0433, Train Accuracy: 98.71%, Validation Loss: 2.3705, Validation Accuracy: 57.48%\n",
      "Epoch 49/100, Train Loss: 0.0416, Train Accuracy: 98.65%, Validation Loss: 2.4834, Validation Accuracy: 55.21%\n",
      "Epoch 50/100, Train Loss: 0.0373, Train Accuracy: 98.65%, Validation Loss: 3.2528, Validation Accuracy: 50.66%\n",
      "Epoch 51/100, Train Loss: 0.0355, Train Accuracy: 98.73%, Validation Loss: 2.6275, Validation Accuracy: 56.52%\n",
      "Epoch 52/100, Train Loss: 0.0393, Train Accuracy: 98.54%, Validation Loss: 2.9541, Validation Accuracy: 53.59%\n",
      "Epoch 53/100, Train Loss: 0.0501, Train Accuracy: 98.37%, Validation Loss: 2.5377, Validation Accuracy: 55.50%\n",
      "Epoch 54/100, Train Loss: 0.0350, Train Accuracy: 98.84%, Validation Loss: 2.5440, Validation Accuracy: 55.50%\n",
      "Epoch 55/100, Train Loss: 0.0372, Train Accuracy: 98.71%, Validation Loss: 2.8222, Validation Accuracy: 54.25%\n",
      "Epoch 56/100, Train Loss: 0.0323, Train Accuracy: 98.95%, Validation Loss: 2.7527, Validation Accuracy: 55.35%\n",
      "Epoch 57/100, Train Loss: 0.0465, Train Accuracy: 98.56%, Validation Loss: 2.5441, Validation Accuracy: 57.33%\n",
      "Epoch 58/100, Train Loss: 0.0415, Train Accuracy: 98.51%, Validation Loss: 2.7195, Validation Accuracy: 56.45%\n",
      "Epoch 59/100, Train Loss: 0.0385, Train Accuracy: 98.71%, Validation Loss: 2.5186, Validation Accuracy: 56.89%\n",
      "Epoch 60/100, Train Loss: 0.0312, Train Accuracy: 98.90%, Validation Loss: 2.6703, Validation Accuracy: 54.69%\n",
      "Epoch 61/100, Train Loss: 0.0327, Train Accuracy: 98.85%, Validation Loss: 2.8013, Validation Accuracy: 53.37%\n",
      "Epoch 62/100, Train Loss: 0.0313, Train Accuracy: 98.92%, Validation Loss: 2.6023, Validation Accuracy: 58.14%\n",
      "Epoch 63/100, Train Loss: 0.0409, Train Accuracy: 98.59%, Validation Loss: 2.7185, Validation Accuracy: 58.50%\n",
      "Epoch 64/100, Train Loss: 0.0355, Train Accuracy: 98.90%, Validation Loss: 2.5259, Validation Accuracy: 55.65%\n",
      "Epoch 65/100, Train Loss: 0.0482, Train Accuracy: 98.68%, Validation Loss: 2.6058, Validation Accuracy: 57.92%\n",
      "Epoch 66/100, Train Loss: 0.0229, Train Accuracy: 99.09%, Validation Loss: 2.8460, Validation Accuracy: 56.23%\n",
      "Epoch 67/100, Train Loss: 0.0278, Train Accuracy: 99.04%, Validation Loss: 2.7306, Validation Accuracy: 57.33%\n",
      "Epoch 68/100, Train Loss: 0.0284, Train Accuracy: 98.90%, Validation Loss: 3.4794, Validation Accuracy: 54.99%\n",
      "Epoch 69/100, Train Loss: 0.0306, Train Accuracy: 98.95%, Validation Loss: 3.0674, Validation Accuracy: 55.35%\n",
      "Epoch 70/100, Train Loss: 0.0324, Train Accuracy: 99.01%, Validation Loss: 3.3535, Validation Accuracy: 54.25%\n",
      "Epoch 71/100, Train Loss: 0.0226, Train Accuracy: 99.29%, Validation Loss: 2.8553, Validation Accuracy: 55.65%\n",
      "Epoch 72/100, Train Loss: 0.0298, Train Accuracy: 98.93%, Validation Loss: 3.1706, Validation Accuracy: 55.57%\n",
      "Epoch 73/100, Train Loss: 0.0319, Train Accuracy: 98.85%, Validation Loss: 2.9543, Validation Accuracy: 56.23%\n",
      "Epoch 74/100, Train Loss: 0.0349, Train Accuracy: 98.98%, Validation Loss: 2.6121, Validation Accuracy: 57.11%\n",
      "Epoch 75/100, Train Loss: 0.0242, Train Accuracy: 99.23%, Validation Loss: 2.7571, Validation Accuracy: 57.11%\n",
      "Epoch 76/100, Train Loss: 0.0375, Train Accuracy: 98.87%, Validation Loss: 2.6850, Validation Accuracy: 57.26%\n",
      "Epoch 77/100, Train Loss: 0.0200, Train Accuracy: 99.42%, Validation Loss: 2.9104, Validation Accuracy: 55.57%\n",
      "Epoch 78/100, Train Loss: 0.0300, Train Accuracy: 98.99%, Validation Loss: 2.9211, Validation Accuracy: 57.48%\n",
      "Epoch 79/100, Train Loss: 0.0155, Train Accuracy: 99.50%, Validation Loss: 2.9945, Validation Accuracy: 57.26%\n",
      "Epoch 80/100, Train Loss: 0.0372, Train Accuracy: 98.79%, Validation Loss: 2.9471, Validation Accuracy: 57.92%\n",
      "Epoch 81/100, Train Loss: 0.0242, Train Accuracy: 99.07%, Validation Loss: 3.2839, Validation Accuracy: 52.86%\n",
      "Epoch 82/100, Train Loss: 0.0269, Train Accuracy: 99.06%, Validation Loss: 3.0880, Validation Accuracy: 54.62%\n",
      "Epoch 83/100, Train Loss: 0.0225, Train Accuracy: 99.18%, Validation Loss: 2.9120, Validation Accuracy: 57.70%\n",
      "Epoch 84/100, Train Loss: 0.0290, Train Accuracy: 99.01%, Validation Loss: 3.1291, Validation Accuracy: 55.79%\n",
      "Epoch 85/100, Train Loss: 0.0182, Train Accuracy: 99.40%, Validation Loss: 3.0000, Validation Accuracy: 58.28%\n",
      "Epoch 86/100, Train Loss: 0.0352, Train Accuracy: 98.87%, Validation Loss: 3.0741, Validation Accuracy: 55.28%\n",
      "Epoch 87/100, Train Loss: 0.0247, Train Accuracy: 99.25%, Validation Loss: 2.9317, Validation Accuracy: 55.94%\n",
      "Epoch 88/100, Train Loss: 0.0203, Train Accuracy: 99.29%, Validation Loss: 2.8962, Validation Accuracy: 57.18%\n",
      "Epoch 89/100, Train Loss: 0.0152, Train Accuracy: 99.53%, Validation Loss: 3.1564, Validation Accuracy: 54.77%\n",
      "Epoch 90/100, Train Loss: 0.0212, Train Accuracy: 99.25%, Validation Loss: 3.1929, Validation Accuracy: 57.26%\n",
      "Epoch 91/100, Train Loss: 0.0344, Train Accuracy: 98.85%, Validation Loss: 3.4905, Validation Accuracy: 53.59%\n",
      "Epoch 92/100, Train Loss: 0.0233, Train Accuracy: 99.23%, Validation Loss: 3.7041, Validation Accuracy: 53.52%\n",
      "Epoch 93/100, Train Loss: 0.0238, Train Accuracy: 99.26%, Validation Loss: 3.4517, Validation Accuracy: 52.71%\n",
      "Epoch 94/100, Train Loss: 0.0184, Train Accuracy: 99.40%, Validation Loss: 3.7544, Validation Accuracy: 51.32%\n",
      "Epoch 95/100, Train Loss: 0.0263, Train Accuracy: 99.15%, Validation Loss: 3.2922, Validation Accuracy: 57.11%\n",
      "Epoch 96/100, Train Loss: 0.0207, Train Accuracy: 99.34%, Validation Loss: 3.2554, Validation Accuracy: 56.16%\n",
      "Epoch 97/100, Train Loss: 0.0227, Train Accuracy: 99.34%, Validation Loss: 3.4180, Validation Accuracy: 54.47%\n",
      "Epoch 98/100, Train Loss: 0.0140, Train Accuracy: 99.51%, Validation Loss: 3.2995, Validation Accuracy: 56.16%\n",
      "Epoch 99/100, Train Loss: 0.0275, Train Accuracy: 99.06%, Validation Loss: 3.3375, Validation Accuracy: 55.43%\n",
      "Epoch 100/100, Train Loss: 0.0179, Train Accuracy: 99.31%, Validation Loss: 3.3896, Validation Accuracy: 55.79%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_params = trial.params\n",
    "\n",
    "class BestConfigCNN(nn.Module):\n",
    "    def __init__(self, params, num_classes, input_shape):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        in_channels = input_shape[0]\n",
    "        for i in range(params['n_conv_layers']):\n",
    "            out_channels = params[f\"conv{i}_out\"]\n",
    "            k = params[f\"conv{i}_k\"]\n",
    "            p = params[f\"conv{i}_pad\"]\n",
    "            self.convs.append(nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=p))\n",
    "            norm_type = params[f\"conv{i}_norm\"]\n",
    "            if norm_type == 'batch':\n",
    "                self.norms.append(nn.BatchNorm2d(out_channels))\n",
    "            elif norm_type == 'layer':\n",
    "                self.norms.append(nn.GroupNorm(1, out_channels))\n",
    "            else:\n",
    "                self.norms.append(None)\n",
    "            in_channels = out_channels\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, *input_shape)\n",
    "            for conv, norm in zip(self.convs, self.norms):\n",
    "                x = conv(x)\n",
    "                if norm: x = norm(x)\n",
    "                x = F.relu(x)\n",
    "                x = self.pool(x)\n",
    "            self.n_flatten = x.numel()\n",
    "\n",
    "        self.fcs = nn.ModuleList()\n",
    "        in_features = self.n_flatten\n",
    "        for j in range(params['n_fc_layers']):\n",
    "            out_feat = params[f\"fc{j}_units\"]\n",
    "            self.fcs.append(nn.Linear(in_features, out_feat))\n",
    "            in_features = out_feat\n",
    "        self.output = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        self.dropout_p = params['dropout']\n",
    "        self.act_name = params['activation']\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x)\n",
    "            if norm:\n",
    "                x = norm(x)\n",
    "            if self.act_name == 'relu':\n",
    "                x = F.relu(x)\n",
    "            elif self.act_name == 'leaky_relu':\n",
    "                x = F.leaky_relu(x)\n",
    "            elif self.act_name == 'elu':\n",
    "                x = F.elu(x)\n",
    "            x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for fc in self.fcs:\n",
    "            x = fc(x)\n",
    "            if self.act_name == 'relu':\n",
    "                x = F.relu(x)\n",
    "            elif self.act_name == 'leaky_relu':\n",
    "                x = F.leaky_relu(x)\n",
    "            elif self.act_name == 'elu':\n",
    "                x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        return self.output(x)\n",
    "\n",
    "sample_batch, _ = next(iter(train_loader))\n",
    "input_shape = sample_batch.shape[1:]\n",
    "model = BestConfigCNN(best_params, num_classes, input_shape).to(device)\n",
    "\n",
    "if best_params['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "elif best_params['optimizer'] == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=best_params['lr'])\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=best_params['lr'], momentum=best_params.get('momentum', 0.9))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "          f\"Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092adce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
