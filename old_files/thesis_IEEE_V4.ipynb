{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9b4d82",
   "metadata": {},
   "source": [
    "# KAN: Kolmogorovâ€“Arnold Networks With ......\n",
    "\n",
    "Implementation of KAN architecture with proper feature extraction and attention mechanisms for MRI image analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f09379",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "\n",
    "Import required libraries and modules for implementing KAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c175f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8335a57",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Setup data pipelines with proper transforms for MRI images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f4fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n",
      "Training samples: 1000\n",
      "Testing samples: 500\n"
     ]
    }
   ],
   "source": [
    "# Data directories\n",
    "TRAIN_DIR = \"/home/mhs/research/thesis/Brain MRI ND-5 Dataset/Training\"\n",
    "TEST_DIR  = \"/home/mhs/research/thesis/Brain MRI ND-5 Dataset/Testing\"\n",
    "\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((64, 64)),                # Resize for backbone\n",
    "    transforms.ToTensor(),                        # Convert to tensor\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Repeat grayscale to 3 channels\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])   # ImageNet normalization\n",
    "])\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(TEST_DIR, transform=transform)\n",
    "\n",
    "# Create subsets properly\n",
    "# Method 1: Random selection of indices\n",
    "train_indices = random.sample(range(len(train_dataset)), min(1000, len(train_dataset)))\n",
    "test_indices = random.sample(range(len(test_dataset)), min(500, len(test_dataset)))\n",
    "\n",
    "# Create subset datasets\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "test_dataset = Subset(test_dataset, test_indices)\n",
    "\n",
    "# Alternative Method 2: Take the first N samples\n",
    "# train_dataset = Subset(train_dataset, list(range(min(1000, len(train_dataset)))))\n",
    "# test_dataset = Subset(test_dataset, list(range(min(500, len(test_dataset)))))\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                         num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True)\n",
    "\n",
    "# When printing class counts, we need to access the original dataset's classes\n",
    "original_train_dataset = datasets.ImageFolder(TRAIN_DIR)\n",
    "num_classes = len(original_train_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e755938",
   "metadata": {},
   "source": [
    "## 3. Define KAN Architecture\n",
    "\n",
    "Implement the Kernel Attention Network model with proper feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b481ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KernelAttention(nn.Module):\n",
    "#     def __init__(self, in_dim, kernel_size=7):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Conv2d(in_dim, in_dim, kernel_size=kernel_size, \n",
    "#                              padding=kernel_size//2, groups=in_dim)\n",
    "#         self.spatial_gate = nn.Sequential(\n",
    "#             nn.Conv2d(in_dim, 1, kernel_size=1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Local feature aggregation\n",
    "#         local_feat = self.conv(x)\n",
    "#         # Generate attention weights\n",
    "#         attn = self.spatial_gate(local_feat)\n",
    "#         return x * attn\n",
    "\n",
    "# class KANModel(nn.Module):\n",
    "#     def __init__(self, num_classes, backbone='resnet18'):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # 1. Feature Extraction Backbone\n",
    "#         if backbone == 'resnet18':\n",
    "#             base = models.resnet18(pretrained=True)\n",
    "#             self.feature_dim = 512\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "            \n",
    "#         # Remove the final FC layer\n",
    "#         self.features = nn.Sequential(*list(base.children())[:-2])\n",
    "        \n",
    "#         # 2. Kernel Attention Module\n",
    "#         self.attention = KernelAttention(self.feature_dim)\n",
    "        \n",
    "#         # 3. Global Average Pooling\n",
    "#         self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "#         # 4. Classifier\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(self.feature_dim, 256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Extract features\n",
    "#         x = self.features(x)  # [B, 512, H', W']\n",
    "        \n",
    "#         # Apply kernel attention\n",
    "#         x = self.attention(x)\n",
    "        \n",
    "#         # Global average pooling\n",
    "#         x = self.gap(x)      # [B, 512, 1, 1]\n",
    "#         x = x.view(x.size(0), -1)  # [B, 512]\n",
    "        \n",
    "#         # Classification\n",
    "#         return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc125d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BSpline(nn.Module):\n",
    "#     \"\"\"B-spline implementation for KAN.\"\"\"\n",
    "\n",
    "#     def __init__(self, in_dim, grid_size=5, degree=3):\n",
    "#         super().__init__()\n",
    "#         self.in_dim = in_dim\n",
    "#         self.grid_size = grid_size\n",
    "#         self.degree = degree\n",
    "\n",
    "#         # Learnable control points - one set for each channel\n",
    "#         self.control_points = nn.Parameter(torch.randn(in_dim, grid_size))\n",
    "\n",
    "#         # Fixed grid points from 0 to 1\n",
    "#         self.register_buffer(\"grid\", torch.linspace(0, 1, grid_size))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x shape: [B, C, N]\n",
    "#         B, C, N = x.shape\n",
    "\n",
    "#         # Ensure C matches in_dim\n",
    "#         assert (\n",
    "#             C == self.in_dim\n",
    "#         ), f\"Input has {C} channels but BSpline expects {self.in_dim}\"\n",
    "\n",
    "#         # Normalize input to [0, 1]\n",
    "#         x_min = x.min(dim=2, keepdim=True)[0]\n",
    "#         x_max = x.max(dim=2, keepdim=True)[0]\n",
    "#         x_norm = (x - x_min) / (x_max - x_min + 1e-8)\n",
    "\n",
    "#         # Initialize output tensor\n",
    "#         out = torch.zeros_like(x)\n",
    "\n",
    "#         # Compute B-spline weights for each channel\n",
    "#         for c in range(C):\n",
    "#             # Expand grid for broadcasting\n",
    "#             grid_expanded = self.grid.view(1, -1)  # [1, grid_size]\n",
    "#             x_expanded = x_norm[:, c, :].unsqueeze(2)  # [B, N, 1]\n",
    "\n",
    "#             # Compute weights using RBF\n",
    "#             weights = torch.exp(\n",
    "#                 -((x_expanded - grid_expanded) ** 2) / 0.1\n",
    "#             )  # [B, N, grid_size]\n",
    "#             weights = weights / (weights.sum(dim=2, keepdim=True) + 1e-8)\n",
    "\n",
    "#             # Apply weights to control points\n",
    "#             out[:, c, :] = torch.matmul(\n",
    "#                 weights, self.control_points[c].unsqueeze(1)\n",
    "#             ).squeeze(2)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class KANAttention(nn.Module):\n",
    "#     def __init__(self, in_dim, grid_size=5, degree=3):\n",
    "#         super().__init__()\n",
    "#         self.in_dim = in_dim\n",
    "\n",
    "#         # Modified KAN layer\n",
    "#         self.spline = BSpline(in_dim, grid_size, degree)\n",
    "#         self.attention_conv = nn.Conv1d(in_dim, 1, 1)\n",
    "#         self.activation = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: [B, C, H, W]\n",
    "#         B, C, H, W = x.shape\n",
    "\n",
    "#         # Reshape to [B, C, H*W]\n",
    "#         x_flat = x.view(B, C, -1)\n",
    "\n",
    "#         # Apply B-spline transformation\n",
    "#         x_spline = self.spline(x_flat)\n",
    "\n",
    "#         # Generate attention weights\n",
    "#         attn = self.attention_conv(x_spline)  # [B, 1, H*W]\n",
    "#         attn = self.activation(attn)\n",
    "\n",
    "#         # Reshape attention back to spatial dimensions\n",
    "#         attn = attn.view(B, 1, H, W)\n",
    "\n",
    "#         # Apply attention\n",
    "#         return x * attn.expand_as(x)\n",
    "\n",
    "\n",
    "# class KANModel(nn.Module):\n",
    "#     def __init__(self, num_classes, backbone=\"mobilenetv3\"):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # 1. Feature Extraction Backbone\n",
    "#         if backbone == \"resnet18\":\n",
    "#             base = models.resnet18(pretrained=True)\n",
    "#             self.feature_dim = 512\n",
    "#             # Remove final FC layer and keep feature extractor\n",
    "#             self.features = nn.Sequential(*list(base.children())[:-2])\n",
    "#         elif backbone == \"mobilenetv3\":\n",
    "#             base = models.mobilenet_v3_small(\n",
    "#                 weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "#             )\n",
    "#             # For MobileNetV3, we need to determine the feature dimension correctly\n",
    "#             self.feature_dim = (\n",
    "#                 576  # MobileNetV3-Small has 576 channels in its last conv layer\n",
    "#             )\n",
    "#             # Replace classifier but keep features\n",
    "#             self.features = base.features\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "\n",
    "#         # 2. KAN Attention Module\n",
    "#         self.attention = KANAttention(self.feature_dim)\n",
    "\n",
    "#         # 3. Global Average Pooling\n",
    "#         self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # 4. Final Classifier\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(self.feature_dim, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Feature extraction\n",
    "#         x = self.features(x)  # [B, feature_dim, H', W']\n",
    "\n",
    "#         # Apply KAN attention\n",
    "#         x = self.attention(x)\n",
    "\n",
    "#         # Global average pooling\n",
    "#         x = self.gap(x)  # [B, feature_dim, 1, 1]\n",
    "#         x = x.view(x.size(0), -1)  # [B, feature_dim]\n",
    "\n",
    "#         # Classification\n",
    "#         return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70488363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KANModel(nn.Module):\n",
    "#     def __init__(self, num_classes, backbone=\"mobilenetv3\"):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # 1. Feature Extraction Backbone\n",
    "#         if backbone == \"resnet18\":\n",
    "#             base = models.resnet18(pretrained=True)\n",
    "#             self.feature_dim = 512\n",
    "#         elif backbone == \"mobilenetv3\":\n",
    "#             base = models.mobilenet_v3_small(\n",
    "#                 weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "#             )\n",
    "#             self.feature_dim = 512\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "\n",
    "#         # Remove final FC layer and keep feature extractor\n",
    "#         self.features = nn.Sequential(*list(base.children())[:-2])\n",
    "\n",
    "#         # 2. KAN Attention Module\n",
    "#         self.attention = KANAttention(self.feature_dim)\n",
    "\n",
    "#         # 3. Global Average Pooling\n",
    "#         self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # 4. Final Classifier\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(self.feature_dim, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Feature extraction\n",
    "#         x = self.features(x)  # [B, 512, H', W']\n",
    "\n",
    "#         # Apply KAN attention\n",
    "#         x = self.attention(x)\n",
    "\n",
    "#         # Global average pooling\n",
    "#         x = self.gap(x)  # [B, 512, 1, 1]\n",
    "#         x = x.view(x.size(0), -1)  # [B, 512]\n",
    "\n",
    "#         # Classification\n",
    "#         return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3629cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialBSpline(nn.Module):\n",
    "    \"\"\"2D B-spline that processes spatial dimensions directly.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, grid_size=5):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # Control points for each input-output channel pair\n",
    "        self.control_points = nn.Parameter(torch.randn(in_channels, out_channels, grid_size * grid_size))\n",
    "        \n",
    "        # Create 2D grid\n",
    "        x = torch.linspace(0, 1, grid_size)\n",
    "        y = torch.linspace(0, 1, grid_size)\n",
    "        xx, yy = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "        grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "        self.register_buffer(\"grid\", grid)  # [grid_size^2, 2]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, C_in, H, W]\n",
    "        B, C_in, H, W = x.shape\n",
    "        \n",
    "        # Create output tensor\n",
    "        out = torch.zeros(B, self.out_channels, H, W, device=x.device)\n",
    "        \n",
    "        # Normalize spatial values to [0,1] for grid lookup\n",
    "        h_norm = torch.linspace(0, 1, H, device=x.device)\n",
    "        w_norm = torch.linspace(0, 1, W, device=x.device)\n",
    "        \n",
    "        # Create normalized 2D position grid for the image\n",
    "        norm_h, norm_w = torch.meshgrid(h_norm, w_norm, indexing=\"ij\")\n",
    "        positions = torch.stack([norm_h.flatten(), norm_w.flatten()], dim=1)  # [H*W, 2]\n",
    "        \n",
    "        # Process each position with B-spline\n",
    "        for c_in in range(C_in):\n",
    "            # Get channel data and normalize to [0,1]\n",
    "            x_c = x[:, c_in]  # [B, H, W]\n",
    "            x_flat = x_c.reshape(B, -1)  # [B, H*W]\n",
    "            x_min = x_flat.min(dim=1, keepdim=True)[0]\n",
    "            x_max = x_flat.max(dim=1, keepdim=True)[0] \n",
    "            x_norm = (x_flat - x_min) / (x_max - x_min + 1e-8)  # [B, H*W]\n",
    "            \n",
    "            # Compute weights based on positions and pixel values\n",
    "            pos_weights = torch.zeros(B, H*W, self.grid_size*self.grid_size, device=x.device)\n",
    "            \n",
    "            for b in range(B):\n",
    "                # Compute spatial distances to grid points\n",
    "                dist_to_grid = torch.cdist(positions, self.grid, p=2)  # [H*W, grid_size^2]\n",
    "                \n",
    "                # Combine with pixel value influence\n",
    "                x_influence = x_norm[b].unsqueeze(1).expand(-1, self.grid_size*self.grid_size)\n",
    "                combined_dist = dist_to_grid * (1.0 + x_influence)\n",
    "                \n",
    "                # RBF kernel\n",
    "                weights = torch.exp(-combined_dist / 0.2)  # [H*W, grid_size^2]\n",
    "                weights = weights / (weights.sum(dim=1, keepdim=True) + 1e-8)\n",
    "                pos_weights[b] = weights\n",
    "            \n",
    "            # Apply weights to control points for each output channel\n",
    "            for c_out in range(self.out_channels):\n",
    "                control_pts = self.control_points[c_in, c_out]  # [grid_size^2]\n",
    "                # Weighted sum for each position\n",
    "                weighted_vals = torch.matmul(pos_weights, control_pts)  # [B, H*W]\n",
    "                out[:, c_out] += weighted_vals.view(B, H, W)\n",
    "                \n",
    "        return out\n",
    "\n",
    "class PureBSplineBlock(nn.Module):\n",
    "    \"\"\"Block using only B-splines with no convolutional layers.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, grid_size=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature transformation with B-splines\n",
    "        self.spatial_spline = SpatialBSpline(\n",
    "            in_channels, out_channels, grid_size=grid_size\n",
    "        )\n",
    "        \n",
    "        # Normalization and activation\n",
    "        # Change this line - normalize only over the channel dimension\n",
    "        self.norm = nn.LayerNorm(out_channels)  \n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # B-spline for attention weights\n",
    "        self.attention_spline = SpatialBSpline(\n",
    "            out_channels, 1, grid_size=grid_size\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        \n",
    "        # Apply spatial B-spline\n",
    "        features = self.spatial_spline(x)\n",
    "        \n",
    "        # Layer normalization across channels\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
    "        features = self.norm(features)  # This now applies across the last dimension only\n",
    "        features = features.permute(0, 3, 1, 2)  # [B, C, H, W]\n",
    "        features = self.act(features)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention = self.attention_spline(features)\n",
    "        attention = self.sigmoid(attention)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        return features * attention\n",
    "\n",
    "class PureKANModel(nn.Module):\n",
    "    \"\"\"Image classifier using only B-splines with NO convolutional layers.\"\"\"\n",
    "    def __init__(self, num_classes, in_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial feature extraction\n",
    "        self.input_proj = nn.Linear(in_channels, 32)\n",
    "        \n",
    "        # B-spline blocks\n",
    "        self.block1 = PureBSplineBlock(32, 64, grid_size=7)\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "        \n",
    "        self.block2 = PureBSplineBlock(64, 128, grid_size=7)\n",
    "        self.pool2 = nn.AvgPool2d(2)\n",
    "        \n",
    "        self.block3 = PureBSplineBlock(128, 256, grid_size=9)\n",
    "        self.pool3 = nn.AvgPool2d(2)\n",
    "        \n",
    "        # Global pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Initial projection\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
    "        x = self.input_proj(x)  # [B, H, W, 32]\n",
    "        x = x.permute(0, 3, 1, 2)  # [B, 32, H, W]\n",
    "        \n",
    "        # Process through B-spline blocks\n",
    "        x = self.block1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c6802",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "Setup training parameters and optimization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbbcdfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = PureKANModel(num_classes=num_classes, in_channels=3).to(device)\n",
    "# Possibly lower learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "\n",
    "# Initialize model\n",
    "# model = KANModel(num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Learning rate scheduler (removed verbose parameter)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409394b9",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation Loop\n",
    "\n",
    "Implement the main training loop with evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0bca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–Œ        | 5/32 [22:24<2:01:02, 268.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     66\u001b[39m val_accs = []\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     train_losses.append(train_loss)\n\u001b[32m     73\u001b[39m     train_accs.append(train_acc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[32m     16\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Track metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/thesis/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc='Training'):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Evaluating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                target_names=train_dataset.classes)\n",
    "    \n",
    "    return avg_loss, accuracy, report\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_acc = 0.0\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Training phase\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, \n",
    "                                      optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Evaluation phase\n",
    "    val_loss, val_acc, val_report = evaluate(model, test_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'kan_best_model.pth')\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Print detailed validation report every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"\\nValidation Report:\")\n",
    "        print(val_report)\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nLoading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load('kan_best_model.pth'))\n",
    "test_loss, test_acc, test_report = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bebc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the total number of trainable parameters in the model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count parameters\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# Display parameter distribution by module\n",
    "param_sizes = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        module_name = name.split('.')[0]\n",
    "        if module_name not in param_sizes:\n",
    "            param_sizes[module_name] = 0\n",
    "        param_sizes[module_name] += param.numel()\n",
    "\n",
    "print(\"\\nParameter distribution by module:\")\n",
    "for module_name, param_count in param_sizes.items():\n",
    "    percentage = 100 * param_count / total_params\n",
    "    print(f\"{module_name}: {param_count:,} parameters ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b2b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modify the training loop to store metrics\n",
    "# train_losses = []\n",
    "# train_accs = []\n",
    "# val_losses = []\n",
    "# val_accs = []\n",
    "\n",
    "# for epoch in range(1, num_epochs + 1):\n",
    "#     # Training phase\n",
    "#     train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "#     train_losses.append(train_loss)\n",
    "#     train_accs.append(train_acc)\n",
    "    \n",
    "#     # Evaluation phase\n",
    "#     val_loss, val_acc, val_report = evaluate(model, test_loader, criterion, device)\n",
    "#     val_losses.append(val_loss)\n",
    "#     val_accs.append(val_acc)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accs, 'b-', label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), val_accs, 'r-', label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
